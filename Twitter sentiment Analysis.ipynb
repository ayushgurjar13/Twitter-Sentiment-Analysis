{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7af0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           clean_text  category\n",
      "0   when modi promised “minimum government maximum...      -1.0\n",
      "1   talk all the nonsense and continue all the dra...       0.0\n",
      "2   what did just say vote for modi  welcome bjp t...       1.0\n",
      "3   asking his supporters prefix chowkidar their n...       1.0\n",
      "4   answer who among these the most powerful world...       1.0\n",
      "5            kiya tho refresh maarkefir comment karo        0.0\n",
      "6   surat women perform yagna seeks divine grace f...       0.0\n",
      "7   this comes from cabinet which has scholars lik...       0.0\n",
      "8   with upcoming election india saga going import...       1.0\n",
      "9                          gandhi was gay does modi         1.0\n",
      "10  things like demonetisation gst goods and servi...       1.0\n",
      "11  hope tuthukudi people would prefer honest well...       1.0\n",
      "12                  calm waters wheres the modi wave        1.0\n",
      "13  one vote can make all the difference anil kapo...       0.0\n",
      "14  one vote can make all the difference anil kapo...       0.0\n",
      "15  vote such party and leadershipwho can take fas...      -1.0\n",
      "16                 vote modi who has not created jobs       0.0\n",
      "17  through our vote ensure govt need and deserve ...       0.0\n",
      "18  dont play with the words was talking about the...       1.0\n",
      "19  didn’ write chowkidar does mean ’ anti modi tr...      -1.0\n",
      "20  was the one who recently said that people who ...       1.0\n",
      "21  with firm belief the leadership shri narendra ...      -1.0\n",
      "22  crush jaws those who shoutmodimodi says jds ml...       0.0\n",
      "23  sultanpur uttar pradesh loksabha candidate sel...      -1.0\n",
      "24  thiugh nehru not alive but still alive heart m...      -1.0\n",
      "25  \\ndevelopment has become mass movement under m...       1.0\n",
      "26  has already taken notice and ordered probe now...       0.0\n",
      "27  was waiting for this modi will also talk about...       0.0\n",
      "28  according yogi imran masood kin azhar masood a...       0.0\n",
      "29  agree but only during the tenure modiganga rej...       0.0\n",
      "30  the three codes modi cracked give india huge f...       1.0\n",
      "31  through our vote ensure govt need and deserve ...       0.0\n",
      "32  modi govts slashing indias education budget cl...      -1.0\n",
      "33  being born religion where female deities worsh...       1.0\n",
      "34  how such people are being made amazedn fear th...      -1.0\n",
      "35                                         only modi        0.0\n",
      "36  check out latest article premier archery leagu...       1.0\n",
      "37  india second most optimistic globally about ex...       1.0\n",
      "38  people wish your vision india and least intere...      -1.0\n",
      "39  modi for eternal what wrong dear sirji perfect...      -1.0\n",
      "40  impressive godrej tata complimenting our hopin...       1.0\n",
      "41  our maid saying this rahul keeps saying modi k...       0.0\n",
      "42  please vote for modi congress trying divide india       0.0\n",
      "43  yes good job highly insensitivearrogant incomp...       1.0\n",
      "44  before 2014 hindustan has seen the worst for h...       1.0\n",
      "45  higher voting turnout directly proportional bj...       1.0\n",
      "46  modi govt has done remarkable job making corru...       1.0\n",
      "47                         use this beg for campaign        0.0\n",
      "48  with welfare delivery gst ibc and feo place mo...       1.0\n",
      "49  this the new india modi trying build with thes...      -1.0\n",
      "50  overpromise and underdelivery – that pithy sum...       1.0\n",
      "51  not just healing touch india need surgery remo...       0.0\n",
      "52  farmers’ welfare about 474 farmers get second ...       0.0\n",
      "53  mistry man not then why drag modi the nri foll...      -1.0\n",
      "54  think you forgot dollar india handled exceptio...       1.0\n",
      "55  entrepreneurs are rising india after modi govt...      -1.0\n",
      "56                       nothing else its modi phobia       0.0\n",
      "57                                        itna fark         0.0\n",
      "58  once again modi government modi govts efforts ...       0.0\n",
      "59  all are with you sir namo again jai hind jai m...       0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset into a pandas DataFrame\n",
    "# Replace 'your_dataset.csv' with the path to your dataset file\n",
    "df = pd.read_csv('Twitter_Data.csv')\n",
    "file=pd.DataFrame(df)\n",
    "file=file.dropna()\n",
    "print(file.head(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd3157f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anantupadhiyay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anantupadhiyay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/anantupadhiyay/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/anantupadhiyay/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom TF-IDF Matrix Shape: (162980, 104322)\n",
      "Sentence after removing stopwords: cat sitting mat . dog playing garden .\n",
      "Shape of the transformed matrix for the new sentence: (1, 104322)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Tokenization and Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Custom stopwords list including default English stopwords and additional stopwords\n",
    "custom_stopwords = [\n",
    "    \"said\", \"say\", \"says\", \"going\", \"like\",\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\",\n",
    "    \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\",\n",
    "    \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\",\n",
    "    \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "]\n",
    "\n",
    "# Load your dataset into a pandas DataFrame\n",
    "# Replace 'your_dataset.csv' with the path to your dataset file\n",
    "df = pd.read_csv('Twitter_Data.csv')\n",
    "file = pd.DataFrame(df)\n",
    "file['clean_text'].fillna('', inplace=True)\n",
    "\n",
    "# Custom TF-IDF Vectorization\n",
    "custom_tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stopwords, tokenizer=tokenize_and_lemmatize)\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix_custom = custom_tfidf_vectorizer.fit_transform(file['clean_text'])\n",
    "\n",
    "# Print the shape of the TF-IDF matrix\n",
    "print(\"Custom TF-IDF Matrix Shape:\", tfidf_matrix_custom.shape)\n",
    "\n",
    "# Define a new sentence\n",
    "new_sentence = \"The cat is sitting on the mat. The dog is playing in the garden.\"\n",
    "\n",
    "# Tokenize the new sentence\n",
    "tokens = word_tokenize(new_sentence.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "tokens_without_stopwords = [token for token in tokens if token not in custom_stopwords]\n",
    "\n",
    "# Join the tokens back into a sentence\n",
    "sentence_without_stopwords = ' '.join(tokens_without_stopwords)\n",
    "\n",
    "# Print the sentence after removing stopwords\n",
    "print(\"Sentence after removing stopwords:\", sentence_without_stopwords)\n",
    "\n",
    "# Transform the new sentence using the custom TF-IDF vectorizer\n",
    "new_sentence_tfidf = custom_tfidf_vectorizer.transform([new_sentence])\n",
    "\n",
    "# Print the shape of the transformed matrix\n",
    "print(\"Shape of the transformed matrix for the new sentence:\", new_sentence_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ac1637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thing', 'like', 'demonetisation', 'gst', 'good', 'and', 'service', 'tax…the', 'upper', 'caste', 'would', 'sort', 'either', 'view', 'favourably', 'say', 'that', 'need', 'give', 'this', 'more', 'time', 'other', 'caste', 'like', 'dalits', 'the', 'muslim', 'were', 'more', 'against', 'because', 'that', '’', 'just', 'not', 'modi', '’', 'constituency2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'things like demonetisation gst goods and services tax…the upper castes would sort either view favourably say that need give this more time other castes like dalits the muslims were more against because that’ just not modi’ constituency2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=file.iloc[10,0]\n",
    "print(tokenize_and_lemmatize(a))\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a5d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (130384, 104322)\n",
      "y_train shape: (130384,)\n",
      "X_test shape: (32596, 104322)\n",
      "y_test shape: (32596,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have your features stored in X and your target variable in y\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix_custom, file['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9831b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file.dropna(subset=['category'], inplace=True)\n",
    "tfidf_matrix_custom = tfidf_matrix_custom[file.index]\n",
    "X = tfidf_matrix_custom\n",
    "y = file['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "filename = 'finalized_svm_m=odel.sav'\n",
    "joblib.dump(svm_model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46b0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33796097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Fit the Random Forest model to the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd17999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Kernel type\n",
    "    'gamma': ['scale', 'auto']  # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=joblib.load(\"finalized_svm_model.sav\")\n",
    "a.predict(new_sentence_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd948b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4249ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a655bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307c67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeed9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
